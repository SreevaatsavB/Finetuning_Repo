{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01efcdee-74e8-4948-a32b-fc85874af265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install pytrec_eval faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caae933-b83d-4136-93da-9e8f6777b621",
   "metadata": {},
   "source": [
    "### Evaluate BGE reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a3be0e-c5e6-4e0b-991e-20feacf123d7",
   "metadata": {},
   "source": [
    "### Evaluate model (With LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c695f58-ad4c-4325-8324-2e83d07e79c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import PeftModel, LoraConfig\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import json\n",
    "\n",
    "def load_trained_model(base_model_id: str, adapter_path: str):\n",
    "    \"\"\"\n",
    "    Load the base model with trained LoRA adapter.\n",
    "    \n",
    "    Args:\n",
    "        base_model_id: Base model identifier (e.g., \"BAAI/bge-m3\")\n",
    "        adapter_path: Path to saved LoRA adapter\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (tokenizer, model)\n",
    "    \"\"\"\n",
    "    print(f\"Loading base model: {base_model_id}\")\n",
    "    \n",
    "    # 4-bit quantization config (same as training)\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        llm_int8_skip_modules=[\"classifier\", \"pre_classifier\"]\n",
    "    )\n",
    "    \n",
    "    # Load base model\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    \n",
    "    # Load LoRA adapter\n",
    "    print(f\"Loading LoRA adapter from: {adapter_path}\")\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rerank_predictions(query: str, documents: List[str], model, tokenizer, \n",
    "                      max_query_length: int = 128, max_passage_length: int = 1024) -> List[Tuple[str, float, int]]:\n",
    "    \"\"\"\n",
    "    Rerank documents for a given query using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        documents: List of documents to rerank\n",
    "        model: The trained reranking model\n",
    "        tokenizer: The tokenizer\n",
    "        max_query_length: Maximum query token length\n",
    "        max_passage_length: Maximum passage token length\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (document, score, rank) sorted by relevance score (descending)\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        return []\n",
    "    \n",
    "    model.eval()\n",
    "    scores = []\n",
    "    \n",
    "    # Add query prompt (same as training)\n",
    "    query_prompt = \"Represent this sentence for searching relevant passages:\"\n",
    "    formatted_query = query_prompt + query\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for doc in documents:\n",
    "            # Tokenize query-document pair exactly like training\n",
    "            qry_inputs = tokenizer.encode(\n",
    "                formatted_query,\n",
    "                truncation=True,\n",
    "                max_length=max_query_length,\n",
    "                add_special_tokens=False\n",
    "            )\n",
    "            doc_inputs = tokenizer.encode(\n",
    "                doc,\n",
    "                truncation=True,\n",
    "                max_length=max_passage_length,\n",
    "                add_special_tokens=False\n",
    "            )\n",
    "            \n",
    "            # Prepare input - same as training data preparation\n",
    "            inputs = tokenizer.prepare_for_model(\n",
    "                qry_inputs,\n",
    "                doc_inputs,\n",
    "                truncation=\"only_second\",\n",
    "                max_length=max_query_length + max_passage_length,\n",
    "                padding=False\n",
    "            )\n",
    "            \n",
    "            # Convert to tensors and ensure proper batch dimension\n",
    "            input_ids = torch.tensor([inputs[\"input_ids\"]], dtype=torch.long)\n",
    "            attention_mask = torch.tensor([inputs[\"attention_mask\"]], dtype=torch.long) if \"attention_mask\" in inputs else None\n",
    "            token_type_ids = torch.tensor([inputs[\"token_type_ids\"]], dtype=torch.long) if \"token_type_ids\" in inputs else None\n",
    "            \n",
    "            # Create final input dict\n",
    "            model_inputs = {\"input_ids\": input_ids}\n",
    "            if attention_mask is not None:\n",
    "                model_inputs[\"attention_mask\"] = attention_mask\n",
    "            if token_type_ids is not None:\n",
    "                model_inputs[\"token_type_ids\"] = token_type_ids\n",
    "            \n",
    "            # Move to device\n",
    "            model_inputs = {k: v.to(model.device) for k, v in model_inputs.items()}\n",
    "            \n",
    "            # Get prediction\n",
    "            outputs = model(**model_inputs)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Apply softmax and get relevance score\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # IMPORTANT: Based on your training code, class 0 is positive (labels = torch.zeros)\n",
    "            # So we should use class 0 probability as relevance score\n",
    "            if logits.shape[-1] == 2:\n",
    "                relevance_score = probs[0, 0].item()  # Class 0 is positive in your training\n",
    "            elif logits.shape[-1] == 1:\n",
    "                relevance_score = torch.sigmoid(logits[0, 0]).item()  # Single output\n",
    "            else:\n",
    "                # If more than 2 classes, use the first one (class 0)\n",
    "                relevance_score = probs[0, 0].item()\n",
    "                \n",
    "            scores.append(relevance_score)\n",
    "    \n",
    "    # Create ranked results\n",
    "    doc_scores = list(zip(documents, scores))\n",
    "    ranked_results = sorted(doc_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Add ranks (1-indexed)\n",
    "    ranked_with_ranks = [(doc, score, rank + 1) for rank, (doc, score) in enumerate(ranked_results)]\n",
    "\n",
    "\n",
    "    return ranked_with_ranks\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_ndcg(relevance_scores: List[int], k: int = None) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain (NDCG).\n",
    "    \n",
    "    Args:\n",
    "        relevance_scores: List of relevance scores (1 for relevant, 0 for non-relevant)\n",
    "        k: Calculate NDCG@k (if None, calculate for all documents)\n",
    "    \n",
    "    Returns:\n",
    "        NDCG score\n",
    "    \"\"\"\n",
    "    if k is not None:\n",
    "        relevance_scores = relevance_scores[:k]\n",
    "    \n",
    "    if not relevance_scores:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate DCG\n",
    "    dcg = relevance_scores[0]  # First document has no discount\n",
    "    for i in range(1, len(relevance_scores)):\n",
    "        dcg += relevance_scores[i] / np.log2(i + 1)\n",
    "    \n",
    "    # Calculate IDCG (ideal DCG)\n",
    "    ideal_relevance = sorted(relevance_scores, reverse=True)\n",
    "    idcg = ideal_relevance[0] if ideal_relevance else 0\n",
    "    for i in range(1, len(ideal_relevance)):\n",
    "        idcg += ideal_relevance[i] / np.log2(i + 1)\n",
    "    \n",
    "    # Return NDCG\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def evaluate_model(eval_dataset: List[Dict], model, tokenizer, shuffle_docs: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a reranking model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        eval_dataset: List of dictionaries with 'query', 'pos', and 'neg' keys\n",
    "        model: The model to evaluate\n",
    "        tokenizer: The tokenizer\n",
    "        shuffle_docs: Whether to shuffle documents to avoid position bias\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all evaluation metrics\n",
    "    \"\"\"\n",
    "    # Eval metrics \n",
    "    correct_at_1 = 0\n",
    "    mrr_sum = 0  # Mean Reciprocal Rank\n",
    "    ndcg_sum = 0  # NDCG\n",
    "    ndcg_at_3_sum = 0  # NDCG@3\n",
    "    ndcg_at_5_sum = 0  # NDCG@5\n",
    "    \n",
    "    print(\"Evaluating model on dataset...\")\n",
    "    for entry in tqdm(eval_dataset, desc=\"Evaluating queries\"):\n",
    "        query = entry[\"query\"]\n",
    "        \n",
    "        # Handle both single positive doc and list of positive docs\n",
    "        if isinstance(entry[\"pos\"], list):\n",
    "            positive_doc = entry[\"pos\"][0]  # Take first positive document\n",
    "        else:\n",
    "            positive_doc = entry[\"pos\"]\n",
    "            \n",
    "        negative_docs = entry[\"neg\"]\n",
    "        \n",
    "        # Combine positive and negative documents\n",
    "        all_docs = [positive_doc] + negative_docs\n",
    "        \n",
    "        # Shuffle documents to avoid position bias\n",
    "        if shuffle_docs:\n",
    "            random.shuffle(all_docs)\n",
    "        \n",
    "        # Rerank the documents\n",
    "        ranked_results = rerank_predictions(query, all_docs, model, tokenizer)\n",
    "        \n",
    "        # Find the rank of the positive document and create relevance list\n",
    "        positive_rank = None\n",
    "        relevance_scores = []\n",
    "        \n",
    "        for doc, score, rank in ranked_results:\n",
    "            # 1 for relevant (positive) document, 0 for non-relevant\n",
    "            relevance = 1 if doc == positive_doc else 0\n",
    "            relevance_scores.append(relevance)\n",
    "            \n",
    "            if doc == positive_doc:\n",
    "                positive_rank = rank\n",
    "        \n",
    "        if positive_rank == 1:\n",
    "            correct_at_1 += 1\n",
    "        \n",
    "        mrr_sum += 1.0 / positive_rank if positive_rank else 0\n",
    "        \n",
    "        # Calculate NDCG metrics\n",
    "        ndcg = calculate_ndcg(relevance_scores)\n",
    "        ndcg_at_3 = calculate_ndcg(relevance_scores, k=3)\n",
    "        ndcg_at_5 = calculate_ndcg(relevance_scores, k=5)\n",
    "        \n",
    "        ndcg_sum += ndcg\n",
    "        ndcg_at_3_sum += ndcg_at_3\n",
    "        ndcg_at_5_sum += ndcg_at_5\n",
    "    \n",
    "    total_queries = len(eval_dataset)\n",
    "    accuracy_at_1 = correct_at_1 / total_queries if total_queries > 0 else 0\n",
    "    mrr = mrr_sum / total_queries if total_queries > 0 else 0\n",
    "    ndcg_avg = ndcg_sum / total_queries if total_queries > 0 else 0\n",
    "    ndcg_at_3_avg = ndcg_at_3_sum / total_queries if total_queries > 0 else 0\n",
    "    ndcg_at_5_avg = ndcg_at_5_sum / total_queries if total_queries > 0 else 0\n",
    "    \n",
    "    results = {\n",
    "        \"total_queries\": total_queries,\n",
    "        \"accuracy_at_1\": accuracy_at_1,\n",
    "        \"mrr\": mrr,\n",
    "        \"ndcg\": ndcg_avg,\n",
    "        \"ndcg_at_3\": ndcg_at_3_avg,\n",
    "        \"ndcg_at_5\": ndcg_at_5_avg\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"Total queries evaluated: {results['total_queries']}\")\n",
    "    print(f\"Accuracy@1: {results['accuracy_at_1']:.4f}\")\n",
    "    print(f\"Mean Reciprocal Rank (MRR): {results['mrr']:.4f}\")\n",
    "    print(f\"NDCG: {results['ndcg']:.4f}\")\n",
    "    print(f\"NDCG@3: {results['ndcg_at_3']:.4f}\")\n",
    "    print(f\"NDCG@5: {results['ndcg_at_5']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def load_evaluation_data(file_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load evaluation dataset from JSON/JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the evaluation dataset file\n",
    "    \n",
    "    Returns:\n",
    "        List of evaluation examples\n",
    "    \"\"\"\n",
    "    eval_data = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        if file_path.endswith('.jsonl'):\n",
    "            for line in f:\n",
    "                eval_data.append(json.loads(line.strip()))\n",
    "        else:\n",
    "            eval_data = json.load(f)\n",
    "    \n",
    "    return eval_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64791dcf-a265-430b-9600-58e154567016",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_evaluation_data(\"ft_data/testing_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46be363e-c598-45f2-b2fe-c2758228c7fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700,\n",
       " {'query': 'What types of companies form the origins of the described global technology leader?',\n",
       "  'pos': ['Our over 50-year history of innovation dates back to our diverse origins from Hewlett-Packard Company, AT&T, LSI Corporation, Broadcom Corporation, Brocade Communications Systems LLC, CA, Inc., Symantec Enterprise Security, and VMware, Inc.'],\n",
       "  'neg': ['Synthroid (levothyroxine sodium tablets, USP) is used in the treatment of hypothyroidism.',\n",
       "   'Our Revolving Credit Agreement contains a net debt-to-EBITDA financial ratio covenant requiring AT&T to maintain, as of the last day of each fiscal quarter, a ratio of not more than 3.75-to-1.',\n",
       "   \"In corporate financial reporting, different sections of the Management's Discussion and Analysis include topics such as Executive Overview, Critical Accounting Policies and Estimates, Results of Operations, and Liquidity and Capital Resources.\",\n",
       "   'For a discussion of legal and other proceedings in which we are involved, see Note 13 - Commitments and Contingencies in the Notes to Consolidated Financial Statements in Part II, Item 8 of this Annual Report on Form 10-K.',\n",
       "   'Net cash provided by operating activities for fiscal 2023 increased when compared to fiscal 2022. The increase is primarily due to moderated levels of inventory purchases, partially offset by a decline in operating income and the timing of certain payments.',\n",
       "   'In July 2015, a number of purported class action antitrust lawsuits were filed alleging that Delta, American, United and Southwest had conspired to restrain capacity.',\n",
       "   'For fiscal 2023, operating expenses as a percentage of net sales increased 23 basis points compared to the previous fiscal year, impacted by charges of $3.3 billion related to opioid-related legal settlements and charges of $0.8 billion related to the reorganization and restructuring of certain businesses in the Walmart International segment, offset by growth in net sales and lower incremental COVID-19 costs.',\n",
       "   'If any of our personnel, representatives, third party vendors or operations are found to violate these or other laws, regulations or requirements, we could suffer additional severe consequences that could have a material adverse effect on our business, results of operations, financial condition and cash flows. The consequences could include, among others, loss of required certifications, suspension or exclusion from or termination of our participation in federal or state government programs, refunds of amounts received in violation of law or applicable payment program requirements, loss of licenses, criminal or civil liability, fines, damages or monetary penalties, and harm to our reputation which could negatively impact our business relationships and stock price.',\n",
       "   'Amazon Web Services (AWS) offers a broad range of on-demand technology services like compute, storage, database, analytics, and machine learning to developers and enterprises of all sizes, including start-ups, government agencies, and academic institutions.']})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data), test_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6a93326-970e-4a54-99a3-25c1b246a07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: BAAI/bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at BAAI/bge-m3 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapter from: bge_m3_reranker_lora_adapter_200\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL_ID = \"BAAI/bge-m3\"\n",
    "ADAPTER_PATH = \"bge_m3_reranker_lora_adapter_200\"  # Path to your saved adapter\n",
    "\n",
    "\n",
    "tokenizer, model = load_trained_model(BASE_MODEL_ID, ADAPTER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd8a9758-5e83-4d29-aca1-60b0fad5ac51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7a556fd-979d-4c6b-aae9-8f21f9bf3f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries:   0%|          | 0/700 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Evaluating queries: 100%|██████████| 700/700 [03:58<00:00,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Total queries evaluated: 700\n",
      "Accuracy@1: 0.9029\n",
      "Mean Reciprocal Rank (MRR): 0.9486\n",
      "NDCG: 0.9940\n",
      "NDCG@3: 0.9911\n",
      "NDCG@5: 0.9940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(test_data, model, tokenizer, shuffle_docs=True)\n",
    "# bsz = 8, 200 steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f5146da-fe30-4197-8c15-c5d0b944aa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 700/700 [03:55<00:00,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Total queries evaluated: 700\n",
      "Accuracy@1: 0.9343\n",
      "Mean Reciprocal Rank (MRR): 0.9667\n",
      "NDCG: 0.9989\n",
      "NDCG@3: 0.9989\n",
      "NDCG@5: 0.9989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(test_data, model, tokenizer, shuffle_docs=True)\n",
    "# bsz = 4, 600 steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1962e-f2ed-4d18-a7dd-587dff3f183d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b23e74-8c2a-4598-a711-b389c0216681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
